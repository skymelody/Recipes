### 介绍下你自己

我叫林志锋, 2020毕业于山东科技大学. 本科学的是计算机专业.  目前就职于CVTE做C++客户端开发. 过往的工作经历主要于音视频编解码有关。



### 介绍音视频同步检测方法

1. 背景

   从视频录制到传输到对端的过程中, 可能由于设备以及网络的原因.导致最终传输到对端的音视频不同步.  在进行音视频同步之前, 我们要看一下延时究竟有多大, 看看是否在可以接受的范围内. 于是, 当时公司就给了我这个任务. 

2. 具体是怎么做的

   首先我们准备一个音视频同步的视频文件. 这个视频文件的音频部分会周期性发出一个声响, 比如说5秒发一声响, 其他时间则不发声. 视频部分则显示视频播放的时间戳. 我们进行视频录制时, 就对这个标准的视频文件进行录制. 录制好之后保存为mp4文件. 

   拿到这个mp4文件后, 我们将音视频数据分离. 对于音频数据, 因为他是周期性发声的. 所以在每一个发声的时间点, 它的sample值是大于0的. 我们可以根据这个sample值算出短时能量, 并记录下这个时间戳, 这个时间戳就是实际录制时的时间戳了. 我们根据这个时间戳去截取视频画面中显示的时间戳, 把它提取出来. 假设录制过程是没有延时的, 那么提取出来的时间戳的值应该是音频声响周期的整数倍. 否则就计算延时. 延时为截取的时间戳减去声响周期整数倍的绝对值.
   
3. Q&A

   * 实现过程中有遇到哪些困难? 怎么解决的?
   
     1. 因为帧率的问题, 有时截到的帧是花的, 对于这种花帧pytesseract识别出来的时间戳也是不正确的. 因此考虑截取的时候截取前后三帧, 如果三帧都是花的, 那么这个时间戳废弃, 并记录到日志. 否则计算时间戳.
     2. 录制过程中, 如果不是在一个完全安静的情况下进行. 那么会存在一些噪音, 这就导致那些能量值本该为0的时间段有了能量. 解决方式是录制的时候找一个相对安静的地方, 这样那些噪音的能量值相比于视频中原有的音频能量值, 即周期性发生的音频来的小很多. 因此我们可以设定一个过滤值, 小于这个值的能量我们就认为是噪音. 
   
   * 为什么用python和c++实现
   
     python实现部分的功能是截取视频帧和图像识别, c++实现的部分的功能是使用ffmpeg分离音视频数据, 计算音频的短时能量. 为什么没用存粹的python或c++实现. 这里主要是结合了python调库的便利性, 以及c++的高效性. 短时能量的计算可用多线程实现, 当时查阅到python的多线程效率很低, 所以就考虑用c++来实现了. 实现后将其编译为动态库供python调用.
     
   * 了解过音视频同步的方法吗？



### 介绍mp4 demuxer

1. Q&A

   * 为什么要自己实现mp4解析而不使用第三方库

     这个是导师要求的, 我觉得应该是为了让我熟悉下这种封装格式吧, 或者是为了看看我的自学能力? 

   * 有遇到什么困难吗?

     解析最终是要提取出h264数据来, 而h264数据在mat box中. 关于mat box的提取有的博客说直接写入文件就好了, 也有的博客说要添加4个字节的0作为前缀, 我都有试过, 但都不能播放成功. 最后我是去看了些ffmpeg的源码才提取出能成功播放的数据.
     
   * 你了解那些封装格式，它们之间的区别是什么？



### 介绍shark2File

1. 背景

   当时有个项目需要和别的公司对接, 我们这里无法很方便的看到传输过去的音视频的质量, 比如说我们能想看看视频帧是否花了之类的. 所以我们采用抓包并提取出音视频数据的方式来检验音视频的效果. 虽然wireshark也有分离音视频数据的功能, 但是当多端通信时, 我需要在wireshark中手动输入IP, 对不同源目的IP进行提取的过程太繁琐了. 所以考虑写一个工具来快速的分离音视频数据

2. 怎么实现的?

   因为公司用不同系统的人都有, 所以考虑把程序写成跨平台的. 这个需求的关键点是对pcap文件进行解析, 当时去github上找了个跨平台的的解析库libpcap. 解析后拿到一个个pcap数据包, 然后提取其中的数据, 首先判断它是否是rtsp数据包. 如果是, 则提取其中的playloadtype字段, 和用户指定的字段做对比来决定是音频数据还是视频数据. 最终根据ip地址将提取的数据写入文件.

3. Q&A

   * 如何判断是否为rtp数据包?
   * 了解rtsp协议吗? 和其他协议对比?



### 介绍带宽优化方案

1. 背景

   我们的这个远程互动助手软件, 对于主讲端来说, 他的画面布局是一个大窗口加上三个小窗口, 大窗口显示的是正在发言人的画面. 对于听讲端来说, 当主讲端发言时, 他的整个屏幕都是主讲端画面, 如果有其他人发言的话, 那么屏幕被一分为二, 分别显示发言人和主讲端的画面. 

   原有的方案, 所有端推送的都是1080p, 2m比特率的视频流. 但我们知道, 对于小窗口, 实际上并不需要这个高的码率, 360p, 500kb码率的视频流就能显示的很清晰了. 对于中等窗口, 540p, 1M的码率就够了. 对于大窗口, 则需要1080p, 1.5M的码率. 所以我们的目的就是设计一种策略, 让各个端推送合适的视频流.

2. 如何实现的?

   在一开始进入房间的时候, 所有听讲端把自己看主讲端, 发言端, 以及看其他端画面大小的请求发送给主讲端. 主讲端收集到这些请求后开始做决策, 对于听课端A来说, 他要推送的画面应该是所有其他端看他画面的最大窗口. 比如说主讲端要看A的小画面, 听课端B要看A中画面, 那么最终A应该推送中画面. 主讲端决策好后将结果发送给各个端. 在之后的过程中, 如果有新的人加入房间, 有人申请发言, 有人结束发言, 有人点击窗口看全屏画面, 都需要将请求发送到主讲端, 主讲端再重新决策, 将结果发送给各个端. 

3. Q&A

   * 有没有考虑过其他的实现方案?

     因为我们当前用的流媒体服务对于上行带宽是不收费的，所以还有一个解决方案是各个端推送不同码率的流， 然后各个端看别人画面时根据需求去拉取相应画面的流。这种方案实现是简单，但性能耗的太多了。除此之外，推送多路流存在者把学校上行带宽占满的可能性。所以当时没有采用这种方案。
    
     

### 介绍Qt窗口16路rtsp流解码渲染

1. 背景

   这个需求其实是另一个软件的， 该软件用于学校领导巡课，巡课的屏幕上要同时显示16路画面，并且每15s就要切到下一组画面。由于和互动助手底层用的SDK是一样的，所以就放在互动助手这里了。原有的实现是SDK把解码后的yuv数据送到前端，前端用Electron来渲染。这种方案对9路流还勉强hold的住。对于16路视频流，cpu的消耗能占到百分之八九十。所以考虑用Qt来重新实现巡课界面。

2. 怎么实现的？

   我采取了多进程的方式实现，主进程接受到前端的rtsp数据后，根据rtsp地址的数量创建相应的窗口数目以及从进程的个数。之后把rtsp地址以及windowId分发给从进程，这里每个从进程默认负责4路rtsp流。从进程拿到rtsp地址后便开始拉流，解码以及渲染。

3. Q&A

   * 为什么采用多进程实现？

     在预研的时候发现单进程内创建的解码渲染器超过12个，进程就会崩溃。这个问题在好几台机子上都测试过了，最终给intel提了issue

   * 进程间是如何通信的？你了解几种进程间通信方式？

     进程间是通过websocket来通信的，从进程创建好后发送一个信息给主进程告知其已经就绪，主进程这时把rtsp流地址和winId发送给对应的从进程。其他的通信方式还有共享内存以及信号量等等。共享内存在互动助手中也有使用，它的作用是将拉流数据拉到的H264流传递给解码渲染进程， 是单向传递的。

   * 进程崩溃问题有考虑吗？

     有，因为解码渲染存在崩溃的可能性。所以在主进程内有个进程管家，进程的创建销毁都是通过进程管家进行的。除此之外，进程管家内还存在一个线程来周期性检测从进程的健康状态。如果有从进程挂了，那么会把挂掉的进程再拉起。当巡课页面关闭后，进程管家销毁所有的从进程。

   * 说说websocket

   





